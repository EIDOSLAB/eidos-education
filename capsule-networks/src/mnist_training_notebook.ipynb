{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch implementation of Capsule Networks\n",
    "\n",
    "Dynamic Routing Between Capsules: https://arxiv.org/abs/1710.09829\n",
    "\n",
    "Author: Riccardo Renzulli\n",
    "University: Universit√† degli Studi di Torino, Department of Computer Science\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import loss.capsule_loss as cl\n",
    "import ops.utils as utils\n",
    "from ops.utils import save_args\n",
    "from dataloaders.load_data import get_dataloader\n",
    "from models.vectorCapsNet import VectorCapsNet\n",
    "from train import train\n",
    "from test import test\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"configs/mnist.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_caps(config):\n",
    "    experiment_folder = utils.create_experiment_folder(config, config.seed)\n",
    "\n",
    "    utils.set_seed(config.seed)\n",
    "\n",
    "    test_base_dir = \"../results/\" + config.dataset + \"/\" + config.model + \"/\" + experiment_folder\n",
    "\n",
    "    logdir = test_base_dir + \"/logs/\"\n",
    "    checkpointsdir = test_base_dir + \"/checkpoints/\"\n",
    "    runsdir = test_base_dir + \"/runs/\"\n",
    "    imgdir = test_base_dir + \"/images/\"\n",
    "\n",
    "    # Make model checkpoint directory\n",
    "    if not os.path.exists(checkpointsdir):\n",
    "        os.makedirs(checkpointsdir)\n",
    "\n",
    "    # Make log directory\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    # Make img directory\n",
    "    if not os.path.exists(imgdir):\n",
    "        os.makedirs(imgdir)\n",
    "\n",
    "    # Set logger path\n",
    "    utils.set_logger(os.path.join(logdir, \"model.log\"))\n",
    "\n",
    "    # Get dataset loaders\n",
    "    train_loader, valid_loader, test_loader = get_dataloader(config)\n",
    "\n",
    "    # Enable GPU usage\n",
    "    if config.use_cuda and torch.cuda.is_available():\n",
    "        device = torch.device(config.cuda_device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    caps_model = VectorCapsNet(config, device)\n",
    "\n",
    "    utils.summary(caps_model, config)\n",
    "\n",
    "    caps_criterion = cl.CapsLoss(config.caps_loss,\n",
    "                                 config.margin_loss_lambda,\n",
    "                                 config.reconstruction_loss_lambda,\n",
    "                                 config.batch_averaged,\n",
    "                                 config.reconstruction is not None,\n",
    "                                 config.m_plus,\n",
    "                                 config.m_minus,\n",
    "                                 config.m_min,\n",
    "                                 config.m_max,\n",
    "                                 device)\n",
    "\n",
    "    if config.optimizer == \"adam\":\n",
    "        caps_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, caps_model.parameters()), lr=config.lr)\n",
    "    else:\n",
    "        caps_optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, caps_model.parameters()), lr=config.lr)\n",
    "    caps_scheduler = torch.optim.lr_scheduler.ExponentialLR(caps_optimizer, config.decay_rate)\n",
    "\n",
    "    caps_model.to(device)\n",
    "\n",
    "    for state in caps_optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "    # Print the model architecture and parameters\n",
    "    utils.summary(caps_model, config)\n",
    "\n",
    "    # Save current settings (hyperparameters etc.)\n",
    "    save_args(config, test_base_dir)\n",
    "\n",
    "    # Writer for TensorBoard\n",
    "    writer = None\n",
    "    if config.tensorboard:\n",
    "        writer = SummaryWriter(runsdir)\n",
    "\n",
    "    # Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "    logging.info(\"Device: {}\".format(device))\n",
    "\n",
    "    logging.info(\"Initial learning rate: {:.4f}\".format(caps_scheduler.get_last_lr()[0]))\n",
    "    logging.info(\"Number of routing iterations: {}\".format(config.num_routing_iterations))\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    epoch = 0\n",
    "    best_epoch = 0\n",
    "    training = True\n",
    "    while training:\n",
    "        # Start training\n",
    "        logging.info(\"Number of routing iterations: {}\".format(caps_model.classCaps.num_iterations))\n",
    "        train(logging, config, train_loader, caps_model, caps_criterion, caps_optimizer, caps_scheduler, writer, epoch, device)\n",
    "\n",
    "        # Start validation\n",
    "        val_loss, val_acc = test(logging, config, valid_loader, caps_model, caps_criterion, writer, epoch, device,\n",
    "                         imgdir, split=\"validation\")\n",
    "        # Start testing\n",
    "        test_loss, test_acc = test(logging, config, test_loader, caps_model, caps_criterion, writer, epoch, device, imgdir, split=\"test\")\n",
    "\n",
    "        \n",
    "        if writer:\n",
    "            writer.add_scalar('routing/iterations', caps_model.classCaps.num_iterations, epoch)\n",
    "            writer.add_scalar('lr', caps_scheduler.get_last_lr()[0], epoch)\n",
    "\n",
    "        formatted_epoch = str(epoch).zfill(len(str(config.epochs - 1)))\n",
    "        checkpoint_filename = \"epoch_{}\".format(formatted_epoch)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            utils.save_checkpoint({\n",
    "                \"epoch\": epoch,\n",
    "                \"routing_iterations\": caps_model.classCaps.num_iterations,\n",
    "                \"state_dict\": caps_model.state_dict(),\n",
    "                \"metric\": config.monitor,\n",
    "                \"optimizer\": caps_optimizer.state_dict(),\n",
    "                \"scheduler\": caps_scheduler.state_dict(),\n",
    "            }, True, checkpointsdir, checkpoint_filename)\n",
    "            best_epoch = epoch\n",
    "            best_loss = val_loss\n",
    "\n",
    "        # Save current epoch checkpoint\n",
    "        utils.save_checkpoint({\n",
    "            \"epoch\": epoch,\n",
    "            \"routing_iterations\": caps_model.classCaps.num_iterations,\n",
    "            \"state_dict\": caps_model.state_dict(),\n",
    "            \"metric\": config.monitor,\n",
    "            \"optimizer\": caps_optimizer.state_dict(),\n",
    "            \"scheduler\": caps_scheduler.state_dict(),\n",
    "        }, False, checkpointsdir, checkpoint_filename, config.dataset==\"mnist\" and config.reconstruction==\"None\" and config.seed==42)\n",
    "        epoch += 1\n",
    "        if epoch - best_epoch > config.patience:\n",
    "            training = False\n",
    "    if writer:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.DotDict(json.load(open(CONFIG_FILE)))\n",
    "config.seed = config.seeds[0]\n",
    "train_test_caps(config)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2b1346c17ac76a205bf2cfe0699deae76659fc9300a9a3d4ec77aae4feaef99"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
